https://www.semanticscholar.org/paper/871037ca207f36ffc5322d7815a2dd58951a6227	Aligning to Social Norms and Values in Interactive Narratives	Published in NAACL 4 May 2022	We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value… 	https://www.aclanthology.org/2022.naacl-main.439.pdf
https://www.semanticscholar.org/paper/21d45b4923ad165fbb6612e08d06f9d786f9b4cc	Symbolic Knowledge Distillation: from General Language Models to Commonsense Models	Published in NAACL 14 October 2021	The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach… 	https://www.aclanthology.org/2022.naacl-main.341.pdf
https://www.semanticscholar.org/paper/400d619cbabeb669115bb7281a889ab869829ef5	MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound	Published 7 January 2022	This task enables it to perform well variety Abstract As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce MERLOT Reserve , a model that represents videos jointly over time – through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives… 	https://arxiv.org/pdf/2201.02639.pdf
https://www.semanticscholar.org/paper/47a67e76ed84260ff19f7a948d764005d1edf1c9	A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge	Published 3 June 2022	The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at… 	https://arxiv.org/pdf/2206.01718.pdf
https://www.semanticscholar.org/paper/56b68a08b404b54354de0df7f18dea34b6536f51	Investigating the Benefits of Free-Form Rationales	Published 25 May 2022	Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We present human studies which show that ECQA rationales indeed provide additional background information to understand a decision, while over 88% of CoS-E rationales do not. Inspired by this finding, we ask… 	https://arxiv.org/pdf/2206.11083.pdf
https://www.semanticscholar.org/paper/e79ee5d836aea7dc80ec6ff6070f64407e8b21cc	ProsocialDialog: A Prosocial Backbone for Conversational Agents	Published 25 May 2022	Warning: this paper discusses and contains content that may be offensive or upsetting. Abstract Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce P ROSOCIAL D IALOG , the ﬁrst large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic… 	https://arxiv.org/pdf/2205.12688.pdf
https://www.semanticscholar.org/paper/83e2c494733e2b60fcd6e3ab9df1df2f36de631d	Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations	Published 24 May 2022	Despite their impressive capabilities, large pretrained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we de-velop M AIEUTIC PROMPTING , which infers a correct answer to a question even from the noisy and… 	https://arxiv.org/pdf/2205.11822.pdf
https://www.semanticscholar.org/paper/9796d9f15957cdabe1d1d11b440385ef2271ba03	Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions	Published 23 May 2022	Generics express generalizations about the world (e.g., “birds can ﬂy"). However, they are not universally true – while sparrows and penguins are both birds, only sparrows can ﬂy and penguins cannot. Commonsense knowledge bases, that are used extensively in many NLP tasks as a source of world-knowledge, can often encode generic knowledge but, by-design, cannot encode such exceptions. It is crucial to realize the speciﬁc instances when a generic statement is true or false. In this work, we… 	https://arxiv.org/pdf/2205.11658.pdf
https://www.semanticscholar.org/paper/12a763cb52f650710900790ca0bc43e5d5b88be6	Generated Knowledge Prompting for Commonsense Reasoning	Published in ACL 15 October 2021	It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base… 	https://www.aclanthology.org/2022.acl-long.225.pdf
https://www.semanticscholar.org/paper/f39eaefc294db4336ee100bd3efc47c2b15684d0	Scarecrow: A Framework for Scrutinizing Machine Text	Published 2021	Modern neural text generation systems can produce remarkably ﬂuent and grammatical texts. While earlier language models suffered from repetition and syntactic errors, the errors made by contemporary models are often semantic, narrative, or discourse failures. To facilitate research of these complex error types, we introduce a new structured, crowdsourced error annotation schema called S CARECROW . The error categories used in S CARECROW —such as redundancy, commonsense errors, and incoherence… 	None
https://www.semanticscholar.org/paper/871037ca207f36ffc5322d7815a2dd58951a6227	Aligning to Social Norms and Values in Interactive Narratives	Published in NAACL 4 May 2022	We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value… 	https://www.aclanthology.org/2022.naacl-main.439.pdf
https://www.semanticscholar.org/paper/4fa63e4b65a13f12440ba976c84535141b6e8332	The Curious Case of Commonsense Intelligence	Published 1 May 2022	Abstract Commonsense intelligence is a long-standing puzzle in AI. Despite considerable advances in deep learning, AI continues to be narrow and brittle due to its lack of common sense. Why is common sense so trivial for humans but so hard for machines? In this essay, I map the twists and turns in recent research adventures toward commonsense AI. As we will see, the latest advances on common sense are riddled with new, potentially counterintuitive perspectives and questions. In particular, I… 	https://doi.org/10.1162/daed_a_01906
https://www.semanticscholar.org/paper/7b0f1d5fa37926ee4bf5a0b530275e45d2a03cf3	Knowledge is Power: Symbolic Knowledge Distillation, Commonsense Morality, & Multimodal Script Knowledge	Published 11 February 2022	Scale appears to be the winning recipe in today's AI leaderboards. And yet, extreme-scale neural models are still brittle to make errors that are often nonsensical and even counterintuitive. In this talk, I will argue for the importance of knowledge, especially commonsense knowledge, and demonstrate how smaller models developed in academia can still have an edge over larger industry-scale models, if powered with knowledge. First, I will introduce "symbolic knowledge distillation", a new… 	http://dl.acm.org/citation.cfm?id=3500242
https://www.semanticscholar.org/paper/Multi-Modal-Answer-Validation-for-Knowledge-Based-Wu-Lu/8dce342a435034fa0521b24b61393397df95c095	Multi-Modal Answer Validation for Knowledge-Based VQA	Published in AAAI 23 March 2021	The problem of knowledge-based visual question answering involves answering questions that require external knowledge in addition to the content of the image. Such knowledge typically comes in various forms, including visual, textual, and commonsense knowledge. Using more knowledge sources increases the chance of retrieving more irrelevant or noisy facts, making it challenging to comprehend the facts and find the answer. To address this challenge, we propose Multi-modal Answer Validation using… 	https://arxiv.org/pdf/2103.12248.pdf
https://www.semanticscholar.org/paper/CommonsenseQA-2.0%3A-Exposing-the-Limits-of-AI-Talmor-Yoran/d65a064eb837f838faf6ff67781b62450b92b159	CommonsenseQA 2.0: Exposing the Limits of AI through Gamification	Published in NeurIPS Datasets and… 7 June 2021	Constructing benchmarks that test the abilities of modern natural language understanding models is difﬁcult – pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamiﬁcation as a framework for data construction. The goal of players in the game is to compose questions that mislead a rival AI, while using speciﬁc phrases for extra points. The… 	https://arxiv.org/pdf/2201.05320.pdf
https://www.semanticscholar.org/paper/MERLOT%3A-Multimodal-Neural-Script-Knowledge-Models-Zellers-Lu/1c6274b1ede93969fc9a5a1d934ad870d66f41ea	MERLOT: Multimodal Neural Script Knowledge Models	Published in NeurIPS 4 June 2021	As humans, we understand events in the visual world contextually, performing multimodal reasoning across time to make inferences about the past, present, and future. We introduce MERLOT, a model that learns multimodal script knowledge by watching millions of YouTube videos with transcribed speech – in an entirely label-free, self-supervised manner. By pretraining with a mix of both framelevel (spatial) and video-level (temporal) objectives, our model not only learns to match images to… 	https://arxiv.org/pdf/2106.02636.pdf
http://api.semanticscholar.org/CorpusID:233289738	Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema	Published in EMNLP 16 April 2021	The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses… 	https://www.aclanthology.org/2021.emnlp-main.819.pdf
https://www.semanticscholar.org/paper/How-Much-Coffee-Was-Consumed-During-EMNLP-2019-A-AI-Kalyan-Kumar/4bbafce8de5301222f236784dfa23217636963e4	How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI	Published 27 October 2021	Many real-world problems require the combined application of multiple reasoning abilities—employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, “How much would the sea… 	https://www.aclanthology.org/2021.emnlp-main.582.pdf
https://www.semanticscholar.org/paper/Probing-Across-Time%3A-What-Does-RoBERTa-Know-and-Liu-Wang/0672f88d5dc762002b515ca4a0a9f101017fea35	Probing Across Time: What Does RoBERTa Know and When?	Published in EMNLP 16 April 2021	Models of language trained on very large corpora have been demonstrated useful for natural language processing. As ﬁxed artifacts, they have become the object of intense study, with many researchers “probing” the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Re-cent work applied several probes to intermediate training stages to observe the develop-mental process of a large-scale model (Chi-ang et al… 	https://arxiv.org/pdf/2104.07885.pdf
https://www.semanticscholar.org/paper/Delphi%3A-Towards-Machine-Ethics-and-Norms-Jiang-Hwang/98eb27ccd9f0875e6e3a350a8a238dc96373a504	Delphi: Towards Machine Ethics and Norms	Published 2021	Failing to account for moral norms could notably hinder AI systems’ ability to interact with people. AI systems empirically require social, cultural, and ethical norms to make moral judgments. However, open-world situations with different groundings may shift moral implications significantly. For example, while “driving my friend to the airport” is “good”, “driving my friend to the airport with a car I stole” is “not okay.” In natural language processing, machine moral reasoning is still in a… 	None
https://www.semanticscholar.org/paper/55d82b6d4fc1ad7c45beb1d2fbcf82fafc5d7f4d	Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules	Published 17 September 2021	One of the challenges faced by conversational agents is their inability to identify unstated presumptions of their users’ commands, a task trivial for humans due to their common sense. In this paper, we propose a zero-shot commonsense reasoning system for conversational agents in an attempt to achieve this. Our reasoner uncovers unstated presumptions from user commands satisfying a general template of if-(state), then-(action), because-(goal). Our reasoner uses a state-of-the-art transformer… 	https://www.aclanthology.org/2021.emnlp-main.588.pdf
https://www.semanticscholar.org/paper/PIGLeT%3A-Language-Grounding-Through-Neuro-Symbolic-a-Zellers-Holtzman/aabeaa2057beee3b1728d0e29248ce642d2027af	PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World	Published in ACL 1 June 2021	We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence… 	https://www.aclanthology.org/2021.acl-long.159.pdf
https://www.semanticscholar.org/paper/Prompting-Contrastive-Explanations-for-Commonsense-Paranjape-Michael/7747ecbc26b1688e6cad1a6ce83914efa2a3c04c	Prompting Contrastive Explanations for Commonsense Reasoning Tasks	Published in FINDINGS 12 June 2021	Many commonsense reasoning NLP tasks involve choosing between one or more possible answers to a question or prompt based on knowledge that is often implicit. Large pretrained language models (PLMs) can achieve near-human performance on such tasks, while providing little human-interpretable evidence of the underlying reasoning they use. In this work, we show how to use these same models to generate such evidence: inspired by the contrastive nature of human explanations, we use PLMs to complete… 	https://www.aclanthology.org/2021.findings-acl.366.pdf
https://www.semanticscholar.org/paper/8bdba45e46471ce23ac2dde24c849623997daaa7	Analyzing Commonsense Emergence in Few-shot Knowledge Models	Published in AKBC 1 January 2021	Recently, commonsense knowledge models — pretrained language models (LM) finetuned on knowledge graph (KG) tuples — showed that considerable amounts of commonsense knowledge can be encoded in the parameters of large language models [Bosselut et al., 2019]. However, as parallel studies show that LMs are poor hypothesizers of declarative commonsense relationships [Petroni et al., 2019] on their own, it remains unclear whether this knowledge is learned during pretraining or from fine-tuning on KG… 	https://arxiv.org/pdf/2101.00297.pdf
https://www.semanticscholar.org/paper/Scarecrow%3A-A-Framework-for-Scrutinizing-Machine-Dou-Forbes/dc4b7f403b4d5a7a192e6cd83b2429da09813a05	Scarecrow: A Framework for Scrutinizing Machine Text	Published 2021	Modern neural text generation systems can produce remarkably ﬂuent and grammatical texts. While earlier language models suffered from repetition and syntactic errors, the errors made by contemporary models are often semantic, narrative, or discourse failures. To facilitate research of these complex error types, we introduce a new structured, crowdsourced error annotation schema called S CARECROW . The error categories used in S CARECROW —such as redundancy, commonsense errors, and incoherence… 	None
https://www.semanticscholar.org/paper/ab847321118de056ff00325d58903b71bdcc76e1	Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text	Published in ACL 2 July 2021	Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called Scarecrow for… 	https://www.aclanthology.org/2022.acl-long.501.pdf
https://www.semanticscholar.org/paper/62953ca1252c9febe07c7007a10911726f37792d	TIMEDIAL: Temporal Commonsense Reasoning in Dialog	Published in ACL 8 June 2021	Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd… 	https://www.aclanthology.org/2021.acl-long.549.pdf
https://www.semanticscholar.org/paper/%22I'm-Not-Mad%22%3A-Commonsense-Implications-of-Negation-Jiang-Bosselut/4137519a1312945537e9ba6b9a631bb00663db83	“I’m Not Mad”: Commonsense Implications of Negation and Contradiction	Published 13 April 2021	Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., “I’m mad at you”), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (“I’m not mad at you”) to commonsense contradictions (“I’m happy”). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For… 	https://www.aclweb.org/anthology/2021.naacl-main.346.pdf
https://www.semanticscholar.org/paper/BERTese%3A-Learning-to-Speak-to-BERT-Haviv-Berant/a49e9a8d29b5838ba392d5d33fb9694f4667c59e	BERTese: Learning to Speak to BERT	Published 9 March 2021	Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manually-authored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into “BERTese”, a paraphrase query that is directly optimized towards better knowledge… 	https://www.aclweb.org/anthology/2021.eacl-main.316.pdf
https://www.semanticscholar.org/paper/19503ec388d7575f1699436318d98d4cbe579fb8	Misinfo Reaction Frames: Reasoning about Readers’ Reactions to News Headlines	Published in ACL 18 April 2021	Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer’s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news.We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to… 	https://www.aclanthology.org/2022.acl-long.222.pdf
https://www.semanticscholar.org/paper/ba40d5fa06a0b14aa50c681c0a38746e348a4491	“I’m Not Mad”: Commonsense Implications of Negation and Contradiction	Published 13 April 2021	Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., “I’m mad at you”), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (“I’m not mad at you”) to commonsense contradictions (“I’m happy”). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For… 	https://www.aclweb.org/anthology/2021.naacl-main.346.pdf
https://www.semanticscholar.org/paper/Information-to-Wisdom%3A-Commonsense-Knowledge-and-Razniewski-Tandon/54df9a3f12fa4810a94a7a5929d0cc7a672b13c4	Information to Wisdom: Commonsense Knowledge Extraction and Compilation	Published 8 March 2021	Commonsense knowledge is a foundational cornerstone of artificial intelligence applications. Whereas information extraction and knowledge base construction for instance-oriented assertions, such as Brad Pitt's birth date, or Angelina Jolie's movie awards, has received much attention, commonsense knowledge on general concepts (politicians, bicycles, printers) and activities (eating pizza, fixing printers) has only been tackled recently. In this tutorial we present state-of-the-art methodologies… 	http://dl.acm.org/citation.cfm?id=3441664
https://www.semanticscholar.org/paper/COMET-ATOMIC-2020%3A-On-Symbolic-and-Neural-Knowledge-Hwang-Bhagavatula/e39503e01ebb108c6773948a24ca798cd444eb62	COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs	Published in AAAI 12 October 2020	Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively… 	https://arxiv.org/pdf/2010.05953.pdf
https://www.semanticscholar.org/paper/Dynamic-Neuro-Symbolic-Knowledge-Graph-Construction-Bosselut-Bras/e2ecce8134a736444065e28a5c12344245b13f7d	Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering	Published in AAAI 1 November 2020	Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge… 	https://ojs.aaai.org/index.php/AAAI/article/view/16625
https://www.semanticscholar.org/paper/Paragraph-Level-Commonsense-Transformers-with-Gabriel-Bhagavatula/5dfc43bb697acf5eacf8b8a05d78dba8beb0dd42	Paragraph-Level Commonsense Transformers with Recurrent Memory	Published in AAAI 4 October 2020	Human understanding of narrative texts requires making commonsense inferences beyond what is stated in the text explicitly. A recent model, COMeT, can generate such inferences along several dimensions such as pre- and post-conditions, motivations, and mental-states of the participants. However, COMeT was trained on short phrases, and is therefore discourse-agnostic. When presented with each sentence of a multi-sentence narrative, it might generate inferences that are inconsistent with the rest… 	https://arxiv.org/pdf/2010.01486.pdf
https://www.semanticscholar.org/paper/UNICORN-on-RAINBOW%3A-A-Universal-Commonsense-Model-a-Lourie-Bras/21ec9c0f869bdb33b06c7dbc8880169db0397d08	UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark	Published in AAAI 24 March 2021	Commonsense AI has long been seen as a near impossible goal—until recently. Now, research interest has sharply increased with an influx of new benchmarks and models. We propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel… 	https://arxiv.org/pdf/2103.13009.pdf
https://www.semanticscholar.org/paper/GENIE%3A-A-Leaderboard-for-Human-in-the-Loop-of-Text-Khashabi-Stanovsky/acf2dd4e2853f90832c01c556a2e716e7c720bc2	GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation	Published 17 January 2021	Leaderboards have eased model development for many NLP datasets by standardizing their evaluation and delegating it to an independent external repository. Their adoption, however, is so far limited to tasks which can be reliably evaluated in an automatic manner. This work introduces GENIE, an extensible human evaluation leaderboard, which brings the ease of leaderboards to text generation tasks. GENIE automatically posts leaderboard submissions to crowdsourcing platforms asking human annotators… 	https://arxiv.org/pdf/2101.06561.pdf
https://www.semanticscholar.org/paper/61deb83ac76069b62337d4a9f2f28b099e5e8d1e	On-the-Fly Attention Modularization for Neural Generation	Published 2021	Despite considerable advancements with deep neural language models (LMs), neural text generation still suffers from degeneration: generated text is repetitive, generic, selfinconsistent, and lacking commonsense. The empirical analyses on sentence-level attention patterns reveal that neural text degeneration may be associated with insufficient learning of inductive biases by the attention mechanism. Our findings motivate on-the-fly attention modularization, a simple but effective method for… 	None
https://www.semanticscholar.org/paper/Do-Neural-Language-Models-Overcome-Reporting-Bias-Shwartz-Choi/7096304d19457833972daec4d3f5107befe30b1c	Do Neural Language Models Overcome Reporting Bias?	Published in COLING 1 December 2020	Mining commonsense knowledge from corpora suffers from reporting bias, over-representing the rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent pre-trained language models overcome this issue. We find that while their generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their… 	https://www.aclweb.org/anthology/2020.coling-main.605.pdf
https://www.semanticscholar.org/paper/Back-to-the-Future%3A-Unsupervised-Backprop-based-for-Qin-Shwartz/14c454c27dffd655cea839a0684a2d855117cd58	Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning	Published 12 October 2020	Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DeLorean, a new… 	https://arxiv.org/pdf/2010.05906.pdf
https://www.semanticscholar.org/paper/CommonGen%3A-A-Constrained-Text-Generation-Challenge-Lin-Shen/fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf	CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning	Published in FINDINGS 14 February 2020	Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee… 	https://www.aclweb.org/anthology/2020.findings-emnlp.165.pdf
https://www.semanticscholar.org/paper/Natural-Language-Rationales-with-Full-Stack-Visual-Marasovi%C4%87-Bhagavatula/c9940a17504a3b83bd1e9d613b095ddb204d2ad0	Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs	Published in FINDINGS 15 October 2020	Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is… 	https://www.aclweb.org/anthology/2020.findings-emnlp.253.pdf
https://www.semanticscholar.org/paper/Social-Chemistry-101%3A-Learning-to-Reason-about-and-Forbes-Hwang/a9a65e600daf6541a1e990924b3b0e13c6cb6e25	Social Chemistry 101: Learning to Reason about Social and Moral Norms	Published 1 November 2020	Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes." 
We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real… 	https://www.aclweb.org/anthology/2020.emnlp-main.48.pdf
https://www.semanticscholar.org/paper/Thinking-Like-a-Skeptic%3A-Defeasible-Inference-in-Rudinger-Shwartz/47e799f83b0850f3d036a2e3a66bb337661b7e68	Thinking Like a Skeptic: Defeasible Inference in Natural Language	Published in FINDINGS 1 November 2020	Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on natural language inference and commonsense reasoning. We introduce Defeasible NLI (abbreviated \delta-NLI), a dataset for defeasible inference in natural language… 	https://www.aclweb.org/anthology/2020.findings-emnlp.418.pdf
https://www.semanticscholar.org/paper/Unsupervised-Commonsense-Question-Answering-with-Shwartz-West/a45b430f057a48b2d4c31c9278248c2b43780bf8	Unsupervised Commonsense Question Answering with Self-Talk	Published 11 April 2020	Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on \emph{self-talk} as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach… 	https://www.aclweb.org/anthology/2020.emnlp-main.373.pdf
https://www.semanticscholar.org/paper/10391eed628dfece8a9136f76c5df53b5704422d	Social Chemistry 101: Learning to Reason about Social and Moral Norms	Published 1 November 2020	Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes." 
We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real… 	https://www.aclweb.org/anthology/2020.emnlp-main.48.pdf
https://www.semanticscholar.org/paper/Generative-Data-Augmentation-for-Commonsense-Yang-Malaviya/dd6f3b6d92ae9448a2000d9690b921f545f00256	Generative Data Augmentation for Commonsense Reasoning	Published 24 April 2020	Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG^C, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using… 	https://pdfs.semanticscholar.org/f7bc/987a820fcf83263f69bd8d938a28d8c5dc44.pdf
https://www.semanticscholar.org/paper/VisualCOMET%3A-Reasoning-about-the-Dynamic-Context-of-Park-Bhagavatula/4aacd623a46adc6a03c925fe3ac007c271c9c6ab	VisualCOMET: Reasoning About the Dynamic Context of a Still Image	Published in ECCV 22 April 2020	Even from a single frame of a still image, people can reason about the dynamic story of the image before, after, and beyond the frame. For example, given an image of a man struggling to stay afloat in water, we can reason that the man fell into the water sometime in the past, the intent of that man at the moment is to stay alive, and he will need help in the near future or else he will get washed away. We propose VisualComet, the novel framework of visual commonsense reasoning tasks to predict… 	https://doi.org/10.1007/978-3-030-58558-7_30
https://www.semanticscholar.org/paper/TransOMCS%3A-From-Linguistic-Graphs-to-Commonsense-Zhang-Khashabi/5fe30f5b788627fd959915cd4fdf9a0b817cbd81	TransOMCS: From Linguistic Graphs to Commonsense Knowledge	Published in IJCAI 1 May 2020	Commonsense knowledge acquisition is a key problem for artificial intelligence. Conventional methods of acquiring commonsense knowledge generally require laborious and costly human annotations, which are not feasible on a large scale. In this paper, we explore a practical way of mining commonsense knowledge from linguistic graphs, with the goal of transferring cheap knowledge obtained with linguistic patterns into expensive commonsense knowledge. The result is a conversion of ASER [Zhang et al… 	https://arxiv.org/pdf/2005.00206.pdf
https://www.semanticscholar.org/paper/Recollection-versus-Imagination%3A-Exploring-Human-Sap-Horvitz/e9e08833787d823dcbec537a04e930f184741471	Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models	Published in ACL 1 July 2020	We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic… 	https://www.aclweb.org/anthology/2020.acl-main.178.pdf
https://www.semanticscholar.org/paper/Social-Bias-Frames%3A-Reasoning-about-Social-and-of-Sap-Gabriel/7b14a165c6b7c1dc2c6c44727e623b94d834fb09	Social Bias Frames: Reasoning about Social and Power Implications of Language	Published 10 November 2019	Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people’s judgments about others. For example, given a statement that “we shouldn’t lower our standards to hire more women,” most listeners will infer the implicature intended by the speaker - that “women (candidates… 	https://www.aclweb.org/anthology/2020.acl-main.486.pdf
https://www.semanticscholar.org/paper/Abductive-Commonsense-Reasoning-Bhagavatula-Bras/a550f576ff20b8cce98f3ddad0043d3783fbc9b4	Abductive Commonsense Reasoning	Published 15 August 2019		https://arxiv.org/pdf/1908.05739.pdf
http://api.semanticscholar.org/8f7133b2e3851b09d659b91e8faa761ec206413f	WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale	Published 24 July 2019	The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense… 	https://arxiv.org/pdf/1907.10641.pdf
https://www.semanticscholar.org/paper/PIQA%3A-Reasoning-about-Physical-Commonsense-in-Bisk-Zellers/04f4e55e14150b7c48b0287ba77c7443df76ed45	PIQA: Reasoning about Physical Commonsense in Natural Language	Published in AAAI 26 November 2019	To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains – such as news articles and encyclopedia entries, where text is plentiful – in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably… 	https://arxiv.org/pdf/1911.11641.pdf
https://api.semanticscholar.org/CorpusID:209439910	Commonsense Knowledge Base Completion with Structural and Semantic Context	Published in AAAI 7 October 2019	Automatic KB completion for commonsense knowledge graphs (e.g., ATOMIC and ConceptNet) poses unique challenges compared to the much studied conventional knowledge bases (e.g., Freebase). Commonsense knowledge graphs use free-form text to represent nodes, resulting in orders of magnitude more nodes compared to conventional KBs ( ∼18x more nodes in ATOMIC compared to Freebase (FB15K-237)). Importantly, this implies significantly sparser graph structures — a major challenge for existing KB… 	https://arxiv.org/pdf/1910.02915.pdf
http://api.semanticscholar.org/arXiv:1909.00277	Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning	Published 31 August 2019		https://www.aclweb.org/anthology/D19-1243.pdf
http://api.semanticscholar.org/arXiv:1909.03065	“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding	Published 6 September 2019	Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to… 	https://www.aclweb.org/anthology/D19-1332.pdf
http://api.semanticscholar.org/arXiv:1904.09728	Social IQA: Commonsense Reasoning about Social Interactions	Published in EMNLP 2019		https://www.aclweb.org/anthology/D19-1454.pdf
http://api.semanticscholar.org/arXiv:1909.04739	WIQA: A dataset for “What if...” reasoning over procedural text	Published 1 September 2019		https://www.aclweb.org/anthology/D19-1629.pdf
https://api.semanticscholar.org/CorpusID:189762527	COMET: Commonsense Transformers for Automatic Knowledge Graph Construction	Published 12 June 2019		https://www.aclweb.org/anthology/P19-1470.pdf
https://api.semanticscholar.org/292161a55562f69e95426ba61897e0e684557f11	HellaSwag: Can a Machine Really Finish Your Sentence?	Published in ACL 1 May 2019		https://www.aclweb.org/anthology/P19-1472.pdf
https://www.semanticscholar.org/paper/cc02386375b1262c3a1d5525154eaea24c761d15	Do Neural Language Representations Learn Physical Commonsense?	Published 1 August 2019		https://arxiv.org/pdf/1908.02899.pdf
https://api.semanticscholar.org/089190eb54104a91a893cff04f478450027addc2	From Recognition to Cognition: Visual Commonsense Reasoning	Published 27 November 2018		http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8953217
http://api.semanticscholar.org/d5084f48212bed80e8c11e1e69669deea3ba2f83	Benchmarking Hierarchical Script Knowledge	Published in NAACL 1 June 2019	Understanding procedural language requires reasoning about both hierarchical and temporal relations between events. For example, “boiling pasta” is a sub-event of “making a pasta dish”, typically happens before “draining pasta,” and requires the use of omitted tools (e.g. a strainer, sink...). While people are able to choose when and how to use abstract versus concrete instructions, the NLP community lacks corpora and tasks for evaluating if our models can do the same. In this paper, we… 	https://www.aclweb.org/anthology/N19-1412.pdf
https://api.semanticscholar.org/3ee37ee4707525cc01950050725c39798f8bfe48	CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge	Published 2019		https://www.aclweb.org/anthology/N19-1421.pdf
https://www.semanticscholar.org/paper/6ff68b34a5f78bdd14437fe5a79aebbc42c26467	DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension	Published 1 February 2019		https://www.aclweb.org/anthology/Q19-1014.pdf
https://www.semanticscholar.org/paper/ATOMIC%3A-An-Atlas-of-Machine-Commonsense-for-If-Then-Sap-Bras/8209a8703d8c48aaca1523cfa307dd1c069e58f3	ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning	Published 2019		https://arxiv.org/pdf/1811.00146.pdf
https://www.semanticscholar.org/paper/QASC%3A-A-Dataset-for-Question-Answering-via-Sentence-Khot-Clark/671b73bc5b64219ad1f28d10d79f3cbda41bb6ac	QASC: A Dataset for Question Answering via Sentence Composition	Published 25 October 2019	Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the… 	https://arxiv.org/pdf/1910.11473.pdf
https://api.semanticscholar.org/arXiv:1808.10012	Reasoning about Actions and State Changes by Injecting Commonsense Knowledge	Published in EMNLP 29 August 2018		https://www.aclweb.org/anthology/D18-1006.pdf
https://api.semanticscholar.org/06c137bffcad7376d5cb4a5f269e2fb88b715647	SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference	Published in EMNLP 16 August 2018		https://www.aclweb.org/anthology/D18-1009.pdf
https://api.semanticscholar.org/arXiv:1805.06939	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	Published in ACL 1 May 2018		https://www.aclweb.org/anthology/P18-1043.pdf
https://api.semanticscholar.org/arXiv:1805.06533	Modeling Naive Psychology of Characters in Simple Commonsense Stories	Published 1 May 2018	Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people’s mental states — a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large… 	https://www.aclweb.org/anthology/P18-1213.pdf
https://api.semanticscholar.org/10.1145/3159652.3159693	VISIR: Visual and Semantic Image Label Refinement	Published 2 February 2018		http://dl.acm.org/citation.cfm?id=3159693
http://api.semanticscholar.org/arXiv:1804.05435	What Happened? Leveraging VerbNet to Predict the Effects of Actions in Procedural Text	Published 15 April 2018	Our goal is to answer questions about paragraphs describing processes (e.g., photosynthesis). Texts of this genre are challenging because the effects of actions are often implicit (unstated), requiring background knowledge and inference to reason about the changing world states. To supply this knowledge, we leverage VerbNet to build a rulebase (called the Semantic Lexicon) of the preconditions and effects of actions, and use it along with commonsense knowledge of persistence to answer questions… 	https://arxiv.org/pdf/1804.05435.pdf
http://api.semanticscholar.org/9940f7489584a7115675ee4e729b3e05ce2ad6cb	Commonsense Knowledge in Machine Intelligence	Published 22 February 2018	There is growing conviction that the future of computing depends on our ability to exploit big data on theWeb to enhance intelligent systems. This includes encyclopedic knowledge for factual details, common sense for human-like reasoning and natural language generation for smarter communication. With recent chatbots conceivably at the verge of passing the Turing Test, there are calls for more common sense oriented alternatives, e.g., the Winograd Schema Challenge. The Aristo QA system… 	http://dl.acm.org/citation.cfm?id=3186562
http://api.semanticscholar.org/a7191c0cf7665b4486ac0f0d167518e35a05c1ee	WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation	Published in ACL 2017	Despite important progress in the area of intelligent systems, most such systems still lack commonsense knowledge that appears crucial for enabling smarter, more human-like decisions. In this paper, we present a system based on a series of algorithms to distill fine-grained disambiguated commonsense knowledge from massive amounts of text. Our WebChild 2.0 knowledge base is one of the largest commonsense knowledge bases available, describing over 2 million disambiguated concepts and activities… 	https://www.aclweb.org/anthology/P17-4020.pdf
https://www.semanticscholar.org/paper/c9f343b492c170c726f607c255ec6c7177dc5800	Verb Physics: Relative Physical Knowledge of Actions and Objects	Published in ACL 12 June 2017		https://www.aclweb.org/anthology/P17-1025.pdf
http://api.semanticscholar.org/3d54fea211981e91b1cba184833679cf08bfe483	Moving beyond the Turing Test with the Allen AI Science Challenge	Published 14 April 2016	Answering questions correctly from standardized eighth-grade science tests is itself a test of machine intelligence. 	http://dl.acm.org/citation.cfm?id=3122814
